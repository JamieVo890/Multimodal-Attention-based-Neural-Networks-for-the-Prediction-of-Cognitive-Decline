{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c71a465",
   "metadata": {},
   "source": [
    "# Multimodal Patch Based networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf87c7",
   "metadata": {},
   "source": [
    "The following model expands on the a patch based architectures by combining feature maps of both MRI and PET modalities using attention based mechanisms. As a part of the ablation study, we also train a model without attention based mechanisms. There are three classifcation stages for the final multimodal attention-based model: \n",
    "- <b>Patch Feature Extraction:</b> \n",
    "    - 27 ResNet models are trained on patches from each patch location to extract local features.\n",
    "- <b>Multimodal Attention:</b>\n",
    "    - For each patch location, we train a model which combines the PET and MRI feature maps at that location using the corresponding patch models.  Multihead attention is used to capture the relationships between both modalities.\n",
    "- <b>Patch fusion:</b>\n",
    "    - Feature maps of the 27 trained attention-patch models are concatenated and used as inputs to a final model for global classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efbc8cd",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00559bb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    " \n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import *\n",
    "import nibabel as nb\n",
    "import torchio as tio\n",
    "\n",
    "from models import (cnn, C3DNet, resnet, ResNetV2, ResNeXt, ResNeXtV2, WideResNet, PreActResNet,\n",
    "        EfficientNet, DenseNet, ShuffleNet, ShuffleNetV2, SqueezeNet, MobileNet, MobileNetV2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9473e0",
   "metadata": {},
   "source": [
    "## Read in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcdafd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject IDs who are progressive normal cognition\n",
    "PNC = pd.read_pickle('PNC.pkl')\n",
    "\n",
    "# Subject IDs who are stable normal cognition\n",
    "SNC = pd.read_pickle('SNC.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ee272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "\n",
    "def read_image_data(input_path):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    ids = []\n",
    "    for filename in sorted(os.listdir(input_path)):\n",
    "        file = os.path.join(input_path, filename)\n",
    "        img_file = nb.load(file)\n",
    "        img = img_file.get_fdata()\n",
    "        X_train.append(img)\n",
    "        ids.append(filename)\n",
    "        # Progressive normal cognition target class 1\n",
    "        if filename[0:8] in np.array(PNC):\n",
    "            y_train.append(1)\n",
    "        # Stable normal cognition class 0\n",
    "        else:\n",
    "            y_train.append(0)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    # Reshape X_train to include channel dimension\n",
    "    X_train = X_train.reshape(X_train.shape + (1,))\n",
    "    return X_train, y_train, ids\n",
    "\n",
    "X_MRI, y_MRI, ids_MRI = read_image_data(\"E:/Work/Processed_MRI/2.MNI_Registration\")\n",
    "X_PET, y_PET, ids_PET = read_image_data(\"E:/Work/Processed_PIB/4.MNI_Registered\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6283bef",
   "metadata": {},
   "source": [
    "## Creating training and test sets for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a32aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.6 - 0.2 - 0.2 train-val-test split\n",
    "X_train_MRI, X_test_MRI, X_train_PET, X_test_PET, ids_train, ids_test, y_train, y_test = train_test_split(X_MRI, \n",
    "                                                                                     X_PET, ids_MRI, \n",
    "                                                                                     y_MRI, test_size=0.2, \n",
    "                                                                                     random_state=101, stratify=y_MRI)\n",
    "\n",
    "X_train_MRI, X_val_MRI, X_train_PET, X_val_PET, ids_train, ids_val, y_train, y_val = train_test_split(X_train_MRI, X_train_PET, \n",
    "                                                                                      ids_train, y_train, \n",
    "                                                                                      test_size=0.25, random_state=101,\n",
    "                                                                                      stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0176a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datasets to tensor format, with channel first\n",
    "train_x_MRI = torch.from_numpy(X_train_MRI).float().permute(0,4,1,2,3)\n",
    "train_x_PET = torch.from_numpy(X_train_PET).float().permute(0,4,1,2,3)\n",
    "train_y = torch.from_numpy(y_train).float()\n",
    "\n",
    "val_x_MRI = torch.from_numpy(X_val_MRI).float().permute(0,4,1,2,3)\n",
    "val_x_PET = torch.from_numpy(X_val_PET).float().permute(0,4,1,2,3)\n",
    "val_y = torch.from_numpy(y_val).float()\n",
    "\n",
    "test_x_MRI = torch.from_numpy(X_test_MRI).float().permute(0,4,1,2,3)\n",
    "test_x_PET = torch.from_numpy(X_test_PET).float().permute(0,4,1,2,3)\n",
    "test_y = torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb06a24",
   "metadata": {},
   "source": [
    "## Perform data augmentation to increase training set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c63c15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def perform_augmentation(dataset, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Define transformations\n",
    "    training_transform = tio.Compose([\n",
    "        tio.RandomAffine(),\n",
    "        tio.RandomElasticDeformation(),\n",
    "        tio.RandomFlip()\n",
    "    ])\n",
    "    augmented_dataset = torch.clone(dataset) \n",
    "    for i in range(len(augmented_dataset)):\n",
    "        augmented_dataset[i] = training_transform(augmented_dataset[i])\n",
    "    return augmented_dataset\n",
    "\n",
    "orig_train_x_MRI = torch.clone(train_x_MRI)\n",
    "orig_train_x_PET = torch.clone(train_x_PET)\n",
    "orig_train_y = torch.clone(train_y)\n",
    "for seed in [1,101,42]:\n",
    "    # Apply transformations and create augmented training set\n",
    "    augmented_train_MRI = perform_augmentation(orig_train_x_MRI, seed)\n",
    "    augmented_train_PET = perform_augmentation(orig_train_x_PET, seed)\n",
    "    \n",
    "    \n",
    "    # Concatenate training and augmented training datasets\n",
    "    train_x_MRI = torch.cat((train_x_MRI, augmented_train_MRI), 0)\n",
    "    train_x_PET = torch.cat((train_x_PET, augmented_train_PET), 0)\n",
    "    train_y = torch.cat((train_y, orig_train_y), 0)\n",
    "\n",
    "# Removing blank space around brain to make dimensions even so we can divide image into uniform patches\n",
    "train_x_MRI = train_x_MRI[:, :, 0:88, 0:108, 0:88]\n",
    "train_x_PET = train_x_PET[:, :, 0:88, 0:108, 0:88]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120176b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load augmented dataset (for reruns)\n",
    "train_x_MRI = torch.load(\"train_x_MRI.pkl\")\n",
    "train_x_PET = torch.load(\"train_x_PET.pkl\")\n",
    "train_y = torch.load(\"train_y.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b303434c",
   "metadata": {},
   "source": [
    "## Divide images into patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66cbbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sample, divide images into 27 uniform 3x3x3 patches of size 44x54x44 with 50% overlap\n",
    "# Then create 27 training datasets for each patch location \n",
    "# e.g a data set for patches of each subject in the top left corner, ..., a dataset for patches of each subject in the middle\n",
    "\n",
    "def create_MRI_PET_patches(images, dim, labels):\n",
    "    # Create datasets for each patch location\n",
    "    datasets_MRI = [[] for _ in range(27)]\n",
    "    datasets_PET = [[] for _ in range(27)]\n",
    "    datasets_MRI_PET = [[] for _ in range(27)]\n",
    "    # Find the starting locations of each patch\n",
    "    starting_points = []\n",
    "    for height_stride in range(3):\n",
    "        for width_stride in range(3):\n",
    "            for depth_stride in range(3):\n",
    "                    start = (0 + height_stride*dim[1]//4, 0 + width_stride*dim[2]//4, 0 + depth_stride*dim[3]//4)\n",
    "                    starting_points.append(start)\n",
    "\n",
    "    # For each image\n",
    "    for i in range(len(images[0])):\n",
    "        # Create patch from every starting point\n",
    "        for j in range(len(starting_points)):\n",
    "            start_pt = starting_points[j]\n",
    "            patch_MRI = images[0][i][:, start_pt[0]:start_pt[0] + dim[1]//2, \n",
    "                                      start_pt[1]:start_pt[1] + dim[2]//2, \n",
    "                                      start_pt[2]:start_pt[2] + dim[3]//2]\n",
    "            \n",
    "            patch_PET = images[1][i][:, start_pt[0]:start_pt[0] + dim[1]//2, \n",
    "                                      start_pt[1]:start_pt[1] + dim[2]//2, \n",
    "                                      start_pt[2]:start_pt[2] + dim[3]//2]\n",
    "            datasets_MRI[j].append(patch_MRI)\n",
    "            datasets_PET[j].append(patch_PET)\n",
    "           \n",
    "            \n",
    "    # For each patch location, stack patches from every sample samples into a tensor as a Tensor Dataset\n",
    "    for i in range(27):\n",
    "        datasets_MRI[i] = torch.stack(datasets_MRI[i])\n",
    "        datasets_PET[i] = torch.stack(datasets_PET[i]) \n",
    "        datasets_MRI_PET[i] = torch.utils.data.TensorDataset(datasets_MRI[i], datasets_PET[i], labels)\n",
    "    return datasets_MRI_PET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aff33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for each patch location\n",
    "\n",
    "dim = train_x_MRI[0].shape\n",
    "patch_train_datasets_MRI_PET = create_MRI_PET_patches((train_x_MRI, train_x_PET), dim, train_y)\n",
    "patch_val_datasets_MRI_PET = create_MRI_PET_patches((val_x_MRI, val_x_PET), dim, val_y)\n",
    "patch_test_datasets_MRI_PET = create_MRI_PET_patches((test_x_MRI, test_x_PET), dim, test_y)\n",
    "\n",
    "\n",
    "# Create data loaders for each patch location dataset\n",
    "patch_train_loaders_MRI_PET = []\n",
    "patch_val_loaders_MRI_PET = []\n",
    "patch_test_loaders_MRI_PET = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603cd852",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "for i in range(27):\n",
    "    patch_train_loaders_MRI_PET.append(torch.utils.data.DataLoader(patch_train_datasets_MRI_PET[i], \n",
    "                                                                  batch_size = batch_size, shuffle = True))\n",
    "    patch_val_loaders_MRI_PET.append(torch.utils.data.DataLoader(patch_val_datasets_MRI_PET[i], \n",
    "                                                                  batch_size = batch_size, shuffle = False))\n",
    "    patch_test_loaders_MRI_PET.append(torch.utils.data.DataLoader(patch_test_datasets_MRI_PET[i], \n",
    "                                                                  batch_size = batch_size, shuffle = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device being used for model training\n",
    "if torch.cuda.is_available(): \n",
    "    dev = \"cuda:0\" \n",
    "else: \n",
    "    dev = \"cpu\" \n",
    "device = torch.device(dev) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e734fb57",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35201530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimiser, error, scheduler, feature_maps=False):\n",
    "    total_train_loss = 0\n",
    "    for MRI_image, PET_image, labels in train_loader:\n",
    "        MRI_image = MRI_image.to(device)\n",
    "        PET_image = PET_image.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Clear gradients\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        # feature_map indicates whether the feature map prior final layer is also included in the outputs of the model\n",
    "        if feature_maps:\n",
    "            outputs = model((MRI_image, PET_image))[1]\n",
    "        else:\n",
    "            outputs = model((MRI_image, PET_image))\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = error(outputs.flatten(), labels)\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Calculating gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Clear GPU Cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = outputs.flatten().round()\n",
    "    print(\"Average Training Loss\", total_train_loss/len(train_loader.dataset))\n",
    "\n",
    "def validate_model(model, val_loader, error, feature_maps=False):\n",
    "    correct_predictions_val = 0\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for MRI_image, PET_image, labels in val_loader:\n",
    "            MRI_image = MRI_image.to(device)\n",
    "            PET_image = PET_image.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward propagation\n",
    "            if feature_maps:\n",
    "                pred = model((MRI_image, PET_image))[1]\n",
    "            else:\n",
    "                pred = model((MRI_image, PET_image))\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = error(pred.flatten(), labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            # Clear GPU Cache\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            preds = pred.flatten().round()\n",
    "            #print(pred)\n",
    "            correct_predictions_val += torch.sum(preds == labels).item()\n",
    "        print(\"Validation Accuracy:\", correct_predictions_val/len(val_loader.dataset))\n",
    "        print(\"Average Validation Loss:\", total_val_loss/len(val_loader.dataset))\n",
    "    return total_val_loss/len(val_loader.dataset)\n",
    "        \n",
    "def evaluate_model(model, test_loader, error, feature_maps=False):\n",
    "    correct_predictions_test = 0\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for MRI_image, PET_image, label in test_loader:\n",
    "            MRI_image = MRI_image.to(device)\n",
    "            PET_image = PET_image.to(device)\n",
    "            \n",
    "            labels.append(label.cpu())\n",
    "            \n",
    "            # Forward propagation\n",
    "            if feature_maps:\n",
    "                pred = model((MRI_image, PET_image))[1]\n",
    "            else:\n",
    "                pred = model((MRI_image, PET_image))\n",
    "\n",
    "            # Clear GPU Cache\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            pred = pred.flatten().round()\n",
    "            preds.append(pred.cpu())\n",
    "            correct_predictions_test += torch.sum(pred.cpu() == label.cpu()).item()\n",
    "    print(\"Test Accuracy:\", correct_predictions_test/len(test_loader.dataset))    \n",
    "    print(\"Recall:\", recall_score(torch.cat(labels), torch.cat(preds)))\n",
    "    print(\"Precision:\", precision_score(torch.cat(labels), torch.cat(preds)))\n",
    "    print(preds)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0952e3",
   "metadata": {},
   "source": [
    "## Stage 1: Patch Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6903ee0",
   "metadata": {},
   "source": [
    "### Load MRI patch models for each patch location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb803ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload weights for rerunning\n",
    "MRI_models = []\n",
    "for i in range(27):\n",
    "    patch_model = ResNetV2.generate_model(\n",
    "        model_depth=10,\n",
    "        n_classes=1,\n",
    "        n_input_channels=1,\n",
    "        shortcut_type='B',\n",
    "        conv1_t_size=7,\n",
    "        conv1_t_stride=2,\n",
    "        no_max_pool=False,\n",
    "        widen_factor=1.0).to(device)\n",
    "    patch_model.load_state_dict(torch.load(f\"./patch_temp/PATCH_TEMP{i}\"))\n",
    "    MRI_models.append(patch_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be0450d",
   "metadata": {},
   "source": [
    "### Load PET patch Models for each patch location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload weights for rerunning\n",
    "PET_models = []\n",
    "for i in range(27):\n",
    "    patch_model = ResNetV2.generate_model(\n",
    "        model_depth=10,\n",
    "        n_classes=1,\n",
    "        n_input_channels=1,\n",
    "        shortcut_type='B',\n",
    "        conv1_t_size=7,\n",
    "        conv1_t_stride=2,\n",
    "        no_max_pool=False,\n",
    "        widen_factor=1.0).to(device)\n",
    "    patch_model.load_state_dict(torch.load(f\"./patch_temp/PET_PATCH_TEMP{i}\"))\n",
    "    PET_models.append(patch_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5294a3e1",
   "metadata": {},
   "source": [
    "## Stage 2 :Multimodal Attention Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49364c89",
   "metadata": {},
   "source": [
    "The models trained below are at the patch level. A model is trained for each patch location. Feature maps from the trained MRI and PET patch models at the same patch location are combined for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multimodal patch model\n",
    "# This combines the features from the individual MRI and PET patch models at a patch location\n",
    "\n",
    "class multimodal_patch_CNN(nn.Module):\n",
    "    def __init__(self, patch_models):\n",
    "        super().__init__()\n",
    "        self.patch_models = nn.ModuleList(patch_models)\n",
    "        self.drop= nn.Dropout(p=0.4)\n",
    "        self.fc1 = nn.Linear(200, 100) \n",
    "        self.rel1 = nn.ReLU()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(100)\n",
    "        \n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        patch_outputs = []\n",
    "        for i in range(2):\n",
    "            patch_output = self.patch_models[i](x[i])[0]  \n",
    "            patch_outputs.append(self.drop(patch_output))\n",
    "        output = torch.cat(patch_outputs, dim=1)\n",
    "        output = self.rel1(self.fc1(output))\n",
    "        features = self.batch_norm1(output)\n",
    "        output = self.sigmoid(self.fc2(features))\n",
    "        return features, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219968da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multimodal patch model wtih Attention based mechanism\n",
    "# This combines the features from the individual MRI and PET patch models at a patch location\n",
    "\n",
    "class multimodal_patch_CNN_attention(nn.Module):\n",
    "    def __init__(self, patch_models):\n",
    "        super().__init__()\n",
    "        self.patch_models = nn.ModuleList(patch_models)\n",
    "        self.drop= nn.Dropout(p=0.4)\n",
    "    \n",
    "        self.att = nn.MultiheadAttention(embed_dim=100, num_heads=4,dropout=0.4)\n",
    "        self.fc1 = nn.Linear(200, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        patch_outputs = []\n",
    "        #attention_patch_outputs = []\n",
    "        for i in range(2):\n",
    "            patch_output = self.patch_models[i](x[i])[0]  \n",
    "            patch_outputs.append(self.drop(patch_output))\n",
    "    \n",
    "        x = torch.stack(patch_outputs)\n",
    "        x, _ = self.att(x,x,x)\n",
    "        x = x.permute(1,0,2)\n",
    "        features = x.reshape(x.shape[0],200)\n",
    "        output = self.sigmoid(self.fc1(features))\n",
    "        return features, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c2b3dc",
   "metadata": {},
   "source": [
    "### Multimodal with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "torch.manual_seed(101)\n",
    "torch.cuda.manual_seed(101)\n",
    "random.seed(101)\n",
    "np.random.seed(101)\n",
    "\n",
    "\n",
    "MRI_PET_patch_models = []\n",
    "\n",
    "# For each patch location, concatenate outputs of trained patch models for each modality and learn local features\n",
    "for i in range(27):\n",
    "    MRI_patch_model = MRI_models[i]\n",
    "    PET_patch_model = PET_models[i]\n",
    "    MRI_PET_patch_model = multimodal_patch_CNN_attention((MRI_patch_model,PET_patch_model)).to(device)\n",
    "    \n",
    "    # Freeze MRI and PET patch model layers (we only want the feature maps)\n",
    "    for name, param in MRI_patch_model.named_parameters():\n",
    "            param.requires_grad = False \n",
    "            \n",
    "    for name, param in PET_patch_model.named_parameters():\n",
    "            param.requires_grad = False \n",
    "    \n",
    "    \n",
    "    # Instantiate dataloaders\n",
    "    MRI_PET_train_dataloaders = patch_train_loaders_MRI_PET[i]\n",
    "    MRI_PET_val_dataloaders = patch_val_loaders_MRI_PET[i]\n",
    "    MRI_PET_test_dataloaders = patch_test_loaders_MRI_PET[i]\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    patience = 15\n",
    "    no_improvement = 0\n",
    "    \n",
    "    # Binary Cross Entropy Loss\n",
    "    error = nn.BCELoss()\n",
    "\n",
    "    # SGD Optimizer\n",
    "    optimiser = SGD(MRI_PET_patch_model.parameters(), lr=0.0001, momentum=0.99)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimiser, start_factor=1.0, end_factor=0.1, total_iters=10)\n",
    "    \n",
    "    for epoch in range(500):\n",
    "        print(f\"----------------------------EPOCH {epoch} PATCH {i}-------------------------------\")\n",
    "        MRI_PET_patch_model.train()\n",
    "        train_model(MRI_PET_patch_model, MRI_PET_train_dataloaders, optimiser, error, scheduler, True)\n",
    "\n",
    "        MRI_PET_patch_model.eval()\n",
    "        val_loss = validate_model(MRI_PET_patch_model, MRI_PET_val_dataloaders, error, True)\n",
    "        \n",
    "        # Save model weights if improvement seen\n",
    "        # Otherwise stop model training if there is no improvement in loss after \"patience\" number of runs\n",
    "        if val_loss <= best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improvement = 0\n",
    "            torch.save(MRI_PET_patch_model.state_dict(), f\"./patch_temp/multimodal_PATCH_TEMP_attention{i}\")\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement <= patience:\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"BEST VAL ACC: {best_val_loss}\")\n",
    "                break\n",
    "         \n",
    "    print(\"----------------------------TEST RESULTS-------------------------------\")\n",
    "    MRI_PET_patch_model.load_state_dict(torch.load(f\"./patch_temp/multimodal_PATCH_TEMP_attention{i}\"))\n",
    "    MRI_PET_patch_model.eval()\n",
    "    evaluate_model(MRI_PET_patch_model, MRI_PET_test_dataloaders, error, True)\n",
    "    MRI_PET_patch_models.append(MRI_PET_patch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab84e06",
   "metadata": {},
   "source": [
    "### Mutlimodal without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5da74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set Seeds\n",
    "torch.manual_seed(101)\n",
    "torch.cuda.manual_seed(101)\n",
    "random.seed(101)\n",
    "np.random.seed(101)\n",
    "\n",
    "\n",
    "MRI_PET_patch_models = []\n",
    "\n",
    "# For each patch location, concatenate outputs of trained patch models for each modality and learn local features\n",
    "for i in range(27):\n",
    "    MRI_patch_model = MRI_models[i]\n",
    "    PET_patch_model = PET_models[i]\n",
    "    MRI_PET_patch_model = multimodal_patch_CNN((MRI_patch_model,PET_patch_model)).to(device)\n",
    "    \n",
    "    # Freeze MRI and PET patch model layers (we only want the feature maps)\n",
    "    for name, param in MRI_patch_model.named_parameters():\n",
    "            param.requires_grad = False \n",
    "            \n",
    "    for name, param in PET_patch_model.named_parameters():\n",
    "            param.requires_grad = False \n",
    "    \n",
    "    # Instantiate dataloaders\n",
    "    MRI_PET_train_dataloaders = patch_train_loaders_MRI_PET[i]\n",
    "    MRI_PET_val_dataloaders = patch_val_loaders_MRI_PET[i]\n",
    "    MRI_PET_test_dataloaders = patch_test_loaders_MRI_PET[i]\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    patience = 15\n",
    "    no_improvement = 0\n",
    "    \n",
    "    # Binary Cross Entropy Loss\n",
    "    error = nn.BCELoss()\n",
    "\n",
    "    # SGD Optimizer\n",
    "    optimiser = SGD(MRI_PET_patch_model.parameters(), lr=0.0001, momentum=0.99)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimiser, start_factor=1.0, end_factor=0.1, total_iters=10)\n",
    "    \n",
    "    for epoch in range(500):\n",
    "        print(f\"----------------------------EPOCH {epoch} PATCH {i}-------------------------------\")\n",
    "        MRI_PET_patch_model.train()\n",
    "        train_model(MRI_PET_patch_model, MRI_PET_train_dataloaders, optimiser, error, scheduler, True)\n",
    "\n",
    "        MRI_PET_patch_model.eval()\n",
    "        val_loss = validate_model(MRI_PET_patch_model, MRI_PET_val_dataloaders, error, True)\n",
    "        \n",
    "        # Save model weights if improvement seen\n",
    "        # Otherwise stop model training if there is no improvement in loss after \"patience\" number of runs\n",
    "        if val_loss <= best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improvement = 0\n",
    "            torch.save(MRI_PET_patch_model.state_dict(), f\"./patch_temp/multimodal_PATCH_TEMP{i}\")\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement <= patience:\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"BEST VAL ACC: {best_val_loss}\")\n",
    "                break\n",
    "         \n",
    "    print(\"----------------------------TEST RESULTS-------------------------------\")\n",
    "    MRI_PET_patch_model.load_state_dict(torch.load(f\"./patch_temp/multimodal_PATCH_TEMP{i}\"))\n",
    "    MRI_PET_patch_model.eval()\n",
    "    evaluate_model(MRI_PET_patch_model, MRI_PET_test_dataloaders, error, True)\n",
    "    MRI_PET_patch_models.append(MRI_PET_patch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b14b7d",
   "metadata": {},
   "source": [
    "# Stage 3. Patch Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe47b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking all 27 patch based datasets to create a single subject level dataset\n",
    "# We will now have a subject level dataset where each row is a subject and their 27 patch images\n",
    "# e.g row 1 would be: [patch_1,....,patch_27] for subject 1\n",
    "\n",
    "all_patch_train_MRI = torch.stack([dataset.tensors[0] for dataset in patch_train_datasets_MRI_PET], dim=1)\n",
    "all_patch_val_MRI = torch.stack([dataset.tensors[0] for dataset in patch_val_datasets_MRI_PET], dim=1)\n",
    "all_patch_test_MRI = torch.stack([dataset.tensors[0] for dataset in patch_test_datasets_MRI_PET], dim=1)\n",
    "\n",
    "all_patch_train_PET = torch.stack([dataset.tensors[1] for dataset in patch_train_datasets_MRI_PET], dim=1)\n",
    "all_patch_val_PET = torch.stack([dataset.tensors[1] for dataset in patch_val_datasets_MRI_PET], dim=1)\n",
    "all_patch_test_PET = torch.stack([dataset.tensors[1] for dataset in patch_test_datasets_MRI_PET], dim=1)\n",
    "\n",
    "all_patch_train_MRI_PET_dataset  = torch.utils.data.TensorDataset(all_patch_train_MRI, all_patch_train_PET, train_y)\n",
    "all_patch_val_MRI_PET_dataset = torch.utils.data.TensorDataset(all_patch_val_MRI, all_patch_val_PET, val_y)\n",
    "all_patch_test_MRI_PET_dataset = torch.utils.data.TensorDataset(all_patch_test_MRI, all_patch_test_PET, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ef3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "all_patch_train_dataloader_MRI_PET = torch.utils.data.DataLoader(all_patch_train_MRI_PET_dataset, \n",
    "                                                                batch_size = batch_size, shuffle = True)\n",
    "all_patch_val_dataloader_MRI_PET = torch.utils.data.DataLoader(all_patch_val_MRI_PET_dataset, \n",
    "                                                                batch_size = batch_size, shuffle = True)\n",
    "all_patch_test_dataloader_MRI_PET = torch.utils.data.DataLoader(all_patch_test_MRI_PET_dataset, \n",
    "                                                                batch_size = batch_size, shuffle = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0f3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our final multimodal attention-based patch network\n",
    "class multimodal_CNN(nn.Module):\n",
    "    def __init__(self, MRI_PET_patch_models):\n",
    "        super().__init__()\n",
    "        self.MRI_PET_patch_models = nn.ModuleList(MRI_PET_patch_models)\n",
    "        self.drop= nn.Dropout(p=0.4)\n",
    "        self.fc3 = nn.Linear(5400, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        patch_outputs = []\n",
    "        for i in range(27):\n",
    "            mri_image = x[0][:, i]\n",
    "            pet_image = x[1][:, i]\n",
    "            patch_output, _ = self.MRI_PET_patch_models[i]((mri_image, pet_image))\n",
    "            patch_outputs.append(self.drop(patch_output))\n",
    "        x = torch.cat(patch_outputs, dim=1)\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1607de0e",
   "metadata": {},
   "source": [
    "### Final Multimodal Patch Model with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57b89f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reload attention weights for rerunning\n",
    "MRI_PET_patch_models = []\n",
    "for i in range(27):\n",
    "    MRI_PET_patch_model = multimodal_patch_CNN_attention((MRI_models[i],PET_models[i])).to(device)\n",
    "    MRI_PET_patch_model.load_state_dict(torch.load(f\"./patch_temp/multimodal_PATCH_TEMP_attention{i}\"))                                           \n",
    "    MRI_PET_patch_models.append(MRI_PET_patch_model)\n",
    "    \n",
    "MRI_patch_model = multimodal_CNN(MRI_PET_patch_models).to(device)\n",
    "\n",
    "# Freeze layers of the patch models (we only want the feature maps)\n",
    "for patch_model in MRI_patch_model.MRI_PET_patch_models:\n",
    "    for name, param in patch_model.named_parameters():\n",
    "        param.requires_grad = False \n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(101)\n",
    "torch.cuda.manual_seed(101)\n",
    "random.seed(101)\n",
    "np.random.seed(101)\n",
    "\n",
    "# Binary Cross Entropy Loss\n",
    "error = nn.BCELoss()\n",
    "\n",
    "# SGD Optimizer\n",
    "optimiser = SGD(MRI_patch_model.parameters(), lr=0.0001, momentum=0.9)\n",
    "scheduler = lr_scheduler.LinearLR(optimiser, start_factor=1.0, end_factor=0.1, total_iters=10)\n",
    "\n",
    "\n",
    "best_val_loss = np.inf\n",
    "patience = 20\n",
    "no_improvement = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    print(f\"----------------------------EPOCH {epoch}-------------------------------\")\n",
    "    MRI_patch_model.train()\n",
    "    train_model(MRI_patch_model, all_patch_train_dataloader_MRI_PET, optimiser, error, scheduler)\n",
    "    \n",
    "    MRI_patch_model.eval()\n",
    "    avg_val_loss = validate_model(MRI_patch_model, all_patch_val_dataloader_MRI_PET, error)\n",
    "    \n",
    "    # Save model weights if improvement seen\n",
    "    # Otherwise stop model training if there is no improvement in loss after \"patience\" number of runs\n",
    "    if avg_val_loss <= best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        no_improvement = 0\n",
    "        torch.save(MRI_patch_model.state_dict(), \"./trained_models_2/MULTIMODAL_MODEL_MODAL_ATTENTION\")\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "        if no_improvement <= patience:\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"BEST VAL LOSS: {best_val_loss}\")\n",
    "            break\n",
    "            \n",
    "print(\"----------------------------TEST RESULTS-------------------------------\")\n",
    "MRI_patch_model.load_state_dict(torch.load(\"./trained_models_2/MULTIMODAL_MODEL_MODAL_ATTENTION\"))\n",
    "MRI_patch_model.eval()\n",
    "evaluate_model(MRI_patch_model, all_patch_test_dataloader_MRI_PET, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735ad909",
   "metadata": {},
   "source": [
    "### Final Multimodal Patch Model with no attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3ca84b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reload no attention weights for rerunning\n",
    "MRI_PET_patch_models = []\n",
    "for i in range(27):\n",
    "    MRI_PET_patch_model = multimodal_patch_CNN((MRI_models[i],PET_models[i])).to(device)\n",
    "    MRI_PET_patch_model.load_state_dict(torch.load(f\"./patch_temp/multimodal_PATCH_TEMP{i}\"))                                           \n",
    "    MRI_PET_patch_models.append(MRI_PET_patch_model)\n",
    "    \n",
    "MRI_patch_model = multimodal_CNN(MRI_PET_patch_models).to(device)\n",
    "\n",
    "# Freeze layers of the patch models (we only want the feature maps)\n",
    "for patch_model in MRI_patch_model.MRI_PET_patch_models:\n",
    "    for name, param in patch_model.named_parameters():\n",
    "        param.requires_grad = False \n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(101)\n",
    "torch.cuda.manual_seed(101)\n",
    "random.seed(101)\n",
    "np.random.seed(101)\n",
    "\n",
    "# Binary Cross Entropy Loss\n",
    "error = nn.BCELoss()\n",
    "\n",
    "# SGD Optimizer\n",
    "optimiser = SGD(MRI_patch_model.parameters(), lr=0.0001, momentum=0.9)\n",
    "scheduler = lr_scheduler.LinearLR(optimiser, start_factor=1.0, end_factor=0.1, total_iters=10)\n",
    "\n",
    "\n",
    "best_val_loss = np.inf\n",
    "patience = 20\n",
    "no_improvement = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    print(f\"----------------------------EPOCH {epoch}-------------------------------\")\n",
    "    MRI_patch_model.train()\n",
    "    train_model(MRI_patch_model, all_patch_train_dataloader_MRI_PET, optimiser, error, scheduler)\n",
    "    \n",
    "    MRI_patch_model.eval()\n",
    "    avg_val_loss = validate_model(MRI_patch_model, all_patch_val_dataloader_MRI_PET, error)\n",
    "    \n",
    "    # Save model weights if improvement seen\n",
    "    # Otherwise stop model training if there is no improvement in loss after \"patience\" number of runs\n",
    "    if avg_val_loss <= best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        no_improvement = 0\n",
    "        torch.save(MRI_patch_model.state_dict(), \"./trained_models_2/MULTIMODAL_MODEL_4\")\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "        if no_improvement <= patience:\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"BEST VAL LOSS: {best_val_loss}\")\n",
    "            break\n",
    "            \n",
    "print(\"----------------------------TEST RESULTS-------------------------------\")\n",
    "MRI_patch_model.load_state_dict(torch.load(\"./trained_models_2/MULTIMODAL_MODEL_4\"))\n",
    "MRI_patch_model.eval()\n",
    "evaluate_model(MRI_patch_model, all_patch_test_dataloader_MRI_PET, error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
